<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>python深度学习入门笔记整理（持续更新） | JOKECHEN'S BLOG</title><meta name="description" content="杂谈"><meta name="keywords" content="程序设计,学习归纳与总结,python"><meta name="author" content="JokeChen"><meta name="copyright" content="JokeChen"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/8.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><meta name="twitter:card" content="summary"><meta name="twitter:title" content="python深度学习入门笔记整理（持续更新）"><meta name="twitter:description" content="杂谈"><meta name="twitter:image" content="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/人工智能.jpg"><meta property="og:type" content="article"><meta property="og:title" content="python深度学习入门笔记整理（持续更新）"><meta property="og:url" content="http://jokechen66.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/"><meta property="og:site_name" content="JOKECHEN'S BLOG"><meta property="og:description" content="杂谈"><meta property="og:image" content="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/人工智能.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://jokechen66.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/"><link rel="next" title="EVA：时代的没落与深意" href="http://jokechen66.github.io/2020/05/06/EVA%EF%BC%9A%E6%97%B6%E4%BB%A3%E7%9A%84%E8%BE%89%E7%85%8C%EF%BC%8C%E6%B2%A1%E8%90%BD%E4%B8%8E%E6%B7%B1%E6%84%8F/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://jokechen6.github.io/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">JOKECHEN'S BLOG</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 分类</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/chiling.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">11</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 分类</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#前言"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">前言</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#神经网络基础"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">神经网络基础</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Logistic回归"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">Logistic回归</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#梯度下降法"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">梯度下降法</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#梯度下降法的具体运用"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">梯度下降法的具体运用</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络基础"><span class="toc-number">2.</span> <span class="toc-text">神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic回归"><span class="toc-number">2.1.</span> <span class="toc-text">Logistic回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降法"><span class="toc-number">2.2.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降法的具体运用"><span class="toc-number">2.3.</span> <span class="toc-text">梯度下降法的具体运用</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/人工智能.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">python深度学习入门笔记整理（持续更新）</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-05-11<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-05-13</time><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.3k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 7 分钟</span><div class="post-meta-pv-cv"></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="110" src="//music.163.com/outchain/player?type=0&id=5014591832&auto=1&height=90"></iframe>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前有过断断续续地学习深度学习的经历<br>对深度学习有一定的了解<br>包括<strong>激活函数，损失函数，卷积，池化</strong>这种基本概念<br>对<strong>CNN，RNN，ResNet</strong>都有一定的了解<br>去年参加的项目里还和队友一起做了个<strong>基于CNN</strong>的智能搜索引擎<br>（没记错的话还花里胡哨地用了点<strong>jieba</strong>分词）<br>不过当时才刚刚大二，知识体系漏洞很大，项目全靠带<br>现在再翻翻当时的源码都得费好大劲才能回想起来在写什么。。。</p>
<p>而想想自己到底学了点什么深度学习，又很难系统地总结出来，东一榔头西一棒，确实很多片面的知识点都会些，但又不深入<br>所以以此契机我决定从头好好梳理一遍深度学习，从最基础的概念开始补全知识漏洞，同时呢就当是对相关的知识也做一个复习（比如<strong>线性代数，概率论，python</strong>之类的）</p>
<p>话不多说，希望能在新一遍的学习中有所收获吧——</p>
<h1 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h1><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p><strong>Logistic回归</strong>主要是适用于分类问题的算法<br>毕竟是入门笔记，这里就只对二元分类做简述并给出概念定义</p>
<p>其基本的线性回归形式为：</p>
<script type="math/tex; mode=display">
y = w^{T}x + b</script><p>当然，用最基础的数学知识来看，这里的 y 取得的是一系列的<strong>实值</strong><br>甚至在理论上值域为 <strong>R</strong><br>而我们期望得到的值域为</p>
<script type="math/tex; mode=display">[0, 1]</script><p>即我们输入一个 x 后，我们需要知道一个概率区间<br>这就需要需要在外层嵌套函数，转变函数的值域<br>最理想的自然是<strong>单位阶跃函数</strong>，但单位阶跃函数一个缺点就是其<strong>不连续</strong>，不能保证<strong>可微</strong>的严格性，所以不能直接使用<br>所以这里需要对单位跃迁函数进行替换，<br>也就是<strong>sigmoid函数</strong>：</p>
<script type="math/tex; mode=display">
y = \frac{1}{1 + e^{-x}}</script><p>也就是说最后可以表示为：</p>
<script type="math/tex; mode=display">
y = \sigma (w^{T}x + b)</script><p>其中</p>
<script type="math/tex; mode=display">
\sigma (x) =  \frac{1}{1 + e^{-x}}</script><p>而当我们在进行神经网络训练时，此时产生的 y’ 只能说是理论值，为了使这个理论值 y’ 和实际值 y 接近，我们需要定义一个损失函数<strong>loss</strong>去衡量 y’ 和 y 之间的误差：<br><strong>注：推导过程可以参考周志华教授的《机器学习》，这里只写结论</strong></p>
<script type="math/tex; mode=display">
L (\widehat{y}, y) = -(ylog\widehat{y} + (1-y)log(1 - \widehat{y}))</script><p>函数的<strong>前一个参数为理论值</strong>， <strong>后一个参数为实际期望值</strong><br>（实际上这就是一个经典的<strong>交叉熵损失函数</strong>）<br>也许有人会觉得为什么不用误差平方进行求值<br>但实际上，至少我所接触的神经网络都是利用<strong>梯度下降法</strong>进行训练的<br>而在梯度下降的过程中会面临很多凸函数问题<br>到那时你就会发现误差平方并不精确，不能有效地找到<strong>局部最小值</strong><br>所以综上，我们选择上述功能相近的<strong>loss函数</strong>作为替换<br>再回看<strong>loss函数</strong>：</p>
<script type="math/tex; mode=display">
L (\widehat{y}, y) = -(ylog\widehat{y} + (1-y)log(1 - \widehat{y}))</script><p>这里并没有标明<strong>log的底数</strong>（并不是默认为10），而实际上 log 的底数并不影响函数的实际含义：<br>我们使用<strong>loss函数</strong>的目的是为了<strong>衡量理论值和实际值的误差</strong><br>所以对上述函数进行分析<br>你会发现当理论值 y = 1 时， y’ 也需要趋于1，反之理论值 y = 0 时 y’ 趋于0亦成立，从而确保了理论值和实际期望值<strong>最大程度上的吻合</strong></p>
<p>明确了<strong>loss</strong>之后，我们还需明确另一个概念<strong>cost</strong><br><strong>loss</strong>是神经网络在单个训练集上的表现<br><strong>cost</strong>则是神经网络在整个训练集上的表现<br>若训练集为：</p>
<script type="math/tex; mode=display">
\left\\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)}) \right\\}</script><p>则<strong>cost</strong>为：</p>
<script type="math/tex; mode=display">
J(w, b) =  \frac{1}{m}\sum_{i = 1}^{m}L (\widehat{y}^{(i)}, y^{(i)})</script><p>w 和 b 就是<strong>Logistic回归</strong>中的参数，在实际训练中应该是作为<strong>超参</strong>处理的</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>在上个子标题中我简单梳理了<strong>Logistic回归</strong>以及 <strong>loss</strong> 和 <strong>cost</strong><br>那么在已知<strong>cost</strong>表达式的情况下，我们对初次训练的神经网络进行参数 w 和 b 的设定，显然，除非你运气够好，不然初次设定的参数<strong>一定是具有较大偏差</strong>的——<br>我们对 w ， b ， J(w, b)  建立空间坐标系，得到如下的空间曲线：<br><strong>注：下图截取自吴恩达老师的深度学习课程</strong><br><a href="https://img-blog.csdnimg.cn/20200512112314827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgxMjgwNA==,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener" data-fancybox="group" data-caption="在这里插入图片描述" class="fancybox"><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20200512112314827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgxMjgwNA==,size_16,color_FFFFFF,t_70#pic_center" class="lazyload" title="在这里插入图片描述"></a><br>为了达到理论值和实际期望值最贴近的状态<br>我们需要 <strong>J(w, b)</strong> 取到局部最小值<br>而实际上对 w 和 b 的初次调参往往会有很大的误差<br>这时候我们就需要<strong>神经网络遵循一套规则渐进找到这个最低点</strong><br>这里使用的就是<strong>梯度下降法</strong></p>
<p>现在具体的解释<strong>梯度下降法</strong>的原理：<br>为了方便研究，我们先将上图的空间坐标系降维至平面坐标系<br><strong>假定 b 是已知确定的</strong><br>此时我们只需研究 w 和 J(w) 的图像：<br><strong>注：下图截取自吴恩达老师的深度学习课程</strong><br><a href="https://img-blog.csdnimg.cn/20200512120506129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgxMjgwNA==,size_16,color_FFFFFF,t_70#pic_center" target="_blank" rel="noopener" data-fancybox="group" data-caption="在这里插入图片描述" class="fancybox"><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20200512120506129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzgxMjgwNA==,size_16,color_FFFFFF,t_70#pic_center" class="lazyload" title="在这里插入图片描述"></a><br>在原图的基础上我添加了红点和蓝点<br>分别对应局部最小值和初次调参值<br>为了使神经网络能渐进地从 B 过渡至 A ，我们对 w 进行以下修正：</p>
<script type="math/tex; mode=display">
w = w - \alpha \frac{dJ(w)}{dw}</script><p>这便是梯度下降法的<strong>核心思路</strong><br>其中：</p>
<script type="math/tex; mode=display">
 \frac{dJ(w)}{dw}</script><p>是函数在当前点的<strong>斜率</strong>，对应了梯度下降的方向<br>而参数 <strong>α</strong> 是学习率，对应了沿当前点的斜率方向下降的深度<br>参数 <strong>α</strong> 非常重要，其取值决定了梯度下降的<strong>效率</strong>：<br>太大则容易错失最低点<br>太小则下降速率过慢，降低了程序执行的速度</p>
<p>当你将 B 点选定在 A 点左侧时，再从公式上理解时：<br>你会发现横坐标在增加<br>但从函数趋势上是在<strong>下降</strong>的，仍然对应了<strong>梯度下降</strong><br>清楚理解平面坐标系的情形后，我们重新回归空间坐标系<br>实际上空间坐标系和平面坐标系建立在完全一致的数学规则上<br>只是需要对 w 和 b 同时进行修正：</p>
<script type="math/tex; mode=display">
w = w - \alpha \frac{\partial J(w, b)}{\partial w}</script><script type="math/tex; mode=display">
b = b - \alpha \frac{\partial J(w, b)}{\partial b}</script><h2 id="梯度下降法的具体运用"><a href="#梯度下降法的具体运用" class="headerlink" title="梯度下降法的具体运用"></a>梯度下降法的具体运用</h2><p>在分别介绍了 <strong>Logistic回归</strong> 和 <strong>梯度下降法</strong> 后<br>我们将两者结合起来审视，看看具体的运作机制<br>首先我们假设我们训练集为：</p>
<script type="math/tex; mode=display">
T = \left\\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)}) \right\\}</script><p>其中：</p>
<script type="math/tex; mode=display">
\left|T\right| = m</script><p>并假设在<strong>Logistic回归</strong>中，特征 w 可表示为集合：</p>
<script type="math/tex; mode=display">
W = \left\\{w_{1}, w_{2}, w_{3},...,w_{n} \right\\}</script><p>使得：</p>
<script type="math/tex; mode=display">
y = w_{1}x_{1} + w_{2}x_{2} +...+ w_{n}x_{n} + b</script><p>我们将此结果赋值给变量 z 以方便后续的变量区分</p>
<script type="math/tex; mode=display">
z = y = w_{1}x_{1} + w_{2}x_{2} +...+ w_{n}x_{n} + b</script><p>并在外层嵌套<strong>sigmoid函数</strong>，使得：</p>
<script type="math/tex; mode=display">
a = \sigma (z)</script><p>依照上文，这里的 a 就是理论值<br>我们再假设实际期望值为 y<br>进而求得：</p>
<script type="math/tex; mode=display">
L (a,  y) = -(yloga + (1-y)log(1 - a))</script><p>运算至此，我们已经求得了<strong>Loss</strong>函数，接下来，我们只需要反推：</p>
<script type="math/tex; mode=display">
\frac{\partial L(a, y)}{\partial w_{i}}</script><p>以获取梯度下降中的关键参数<br>其实对于有一定微积分基础的人而言，这个反推过程并不复杂<br>（其实就是反向传播）<br>只需要对<strong>链式规则</strong>加以利用即可：</p>
<script type="math/tex; mode=display">
\frac{\partial L(a, y)}{\partial w_{i}} = \frac{\partial L(a, y)}{\partial a}\times \frac{\partial a}{\partial z}\times\frac{\partial z}{\partial w_{i}}</script><p>省略具体的偏导过程，我们最终求出结果：</p>
<script type="math/tex; mode=display">
\frac{\partial L(a, y)}{\partial w_{i}} = x_{i}\times(a - y)</script><p>而实际上，在上一个标题中，梯度下降公式中对应的参数为：</p>
<script type="math/tex; mode=display">
\frac{\partial J(w, b)}{\partial w}</script><p>也就意味着我们需要对<strong>整个训练集</strong>进行处理，从而求出该参数，<br>定义额外变量如下：</p>
<script type="math/tex; mode=display">
J = 0; w_{1} = 0; w_{2} = 0;...;w_{n};b = 0;</script><p><strong>遍历</strong>整个训练集，有：</p>
<script type="math/tex; mode=display">
J += L (a^{(i)},  y^{(i)})</script><script type="math/tex; mode=display">
w_{i} +=x_{i}^{(i)}\times(a^{(i)} - y^{(i)})</script><script type="math/tex; mode=display">
b +=(a^{(i)} - y^{(i)})</script><p>并对整个训练集取平均：</p>
<script type="math/tex; mode=display">
J = \frac{J}{m}</script><p><strong>对其余设定的额外变量做同样的取平均处理</strong><br>由此我们可以求得：</p>
<script type="math/tex; mode=display">
w_{i} = \frac{\partial J}{\partial w_{i}}</script><p>同理：</p>
<script type="math/tex; mode=display">
b= \frac{\partial J}{\partial b}</script><p>并带入最终的<strong>梯度下降公式</strong>中<br>就可以对输入的各个参数作出一次修正了</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">JokeChen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://jokechen66.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/">http://jokechen66.github.io/2020/05/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://jokechen66.github.io">JOKECHEN'S BLOG</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/">程序设计    </a><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E5%BD%92%E7%BA%B3%E4%B8%8E%E6%80%BB%E7%BB%93/">学习归纳与总结    </a><a class="post-meta__tags" href="/tags/python/">python    </a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/人工智能.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/05/06/EVA%EF%BC%9A%E6%97%B6%E4%BB%A3%E7%9A%84%E8%BE%89%E7%85%8C%EF%BC%8C%E6%B2%A1%E8%90%BD%E4%B8%8E%E6%B7%B1%E6%84%8F/"><img class="next_cover lazyload" data-src="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/EVA.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>EVA：时代的没落与深意</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/31/不完全信息博弈学习笔记/" title="麻将AI & 不完全信息博弈学习笔记（完结）"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/AI.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-31</div><div class="relatedPosts_title">麻将AI & 不完全信息博弈学习笔记（完结）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/13/基于hexo的博客搭建指北/" title="基于hexo的博客搭建指北（持续更新）"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/JOKECHEN66/cloudimg/master/data/cover2.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-13</div><div class="relatedPosts_title">基于hexo的博客搭建指北（持续更新）</div></div></a></div></div><div class="clear_both"></div></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By JokeChen</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script id="ribbon_piao" mobile="true" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/piao.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>